# Production Readiness Roadmap

**í”„ë¡œì íŠ¸**: Real-time CTR Pipeline
**ëª©í‘œ**: Toy Project â†’ Production-Grade Data Pipeline
**ì‘ì„±ì¼**: 2025-12-07
**ëŒ€ìƒ**: 5ë…„ì°¨ ë°±ì—”ë“œ ê°œë°œì â†’ ë°ì´í„° ì—”ì§€ë‹ˆì–´ ì „í™˜

---

## ê°œìš”

ì´ ë¬¸ì„œëŠ” í˜„ì¬ í”„ë¡œì íŠ¸ë¥¼ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬ ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ë¡œë“œë§µì…ë‹ˆë‹¤.

### í˜„ì¬ ìƒíƒœ
- **ì•„í‚¤í…ì²˜**: 8/10 (ìš°ìˆ˜)
- **ì½”ë“œ í’ˆì§ˆ**: 7/10 (ì–‘í˜¸)
- **í…ŒìŠ¤íŠ¸**: 4/10 (ì‹¬ê°í•˜ê²Œ ë¶€ì¡±)
- **ìš´ì˜ ì„±ìˆ™ë„**: 5/10 (ë¯¸í¡)
- **í”„ë¡œë•ì…˜ ì¤€ë¹„ë„**: **40%**

### ëª©í‘œ ìƒíƒœ
- **í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€**: 80%+
- **ë³´ì•ˆ**: ì¸ì¦/ì•”í˜¸í™” ì™„ë£Œ
- **ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ + ì•Œë¦¼
- **CI/CD**: ìë™í™”ëœ ë°°í¬ íŒŒì´í”„ë¼ì¸
- **í”„ë¡œë•ì…˜ ì¤€ë¹„ë„**: **90%+**

---

## ìš°ì„ ìˆœìœ„ ë ˆë²¨ ì •ì˜

| ë ˆë²¨ | ì„¤ëª… | ê¸°ì¤€ |
|------|------|------|
| **P0 - Critical** | ì¦‰ì‹œ ìˆ˜ì • í•„ìš”, í”„ë¡œë•ì…˜ ë°°í¬ ì°¨ë‹¨ ìš”ì†Œ | ë°ì´í„° ì†ì‹¤/ë³´ì•ˆ ìœ„í—˜ |
| **P1 - High** | í”„ë¡œë•ì…˜ ë°°í¬ ì „ í•„ìˆ˜ | ìš´ì˜ ë¶ˆê°€ëŠ¥ |
| **P2 - Medium** | í”„ë¡œë•ì…˜ í’ˆì§ˆ í–¥ìƒ | ìš´ì˜ íš¨ìœ¨ì„± |
| **P3 - Low** | Nice to have | ìµœì í™”/í¸ì˜ì„± |

---

## Phase 1: Critical Fixes (1-2ì£¼)

**ëª©í‘œ**: ì‹¬ê°í•œ ë²„ê·¸ ìˆ˜ì • ë° ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¸í”„ë¼ êµ¬ì¶•
**ì™„ë£Œ ì¡°ê±´**: ëª¨ë“  P0 ì´ìŠˆ í•´ê²°, í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê°€ëŠ¥

### P0-1: CTRResultWindowProcessFunction ë²„ê·¸ ìˆ˜ì •

**íŒŒì¼**: `flink-app/src/main/kotlin/com/example/ctr/domain/service/CTRResultWindowProcessFunction.kt:18`

**í˜„ì¬ ë¬¸ì œ**:
```kotlin
val counts = elements.iterator().next()  // ğŸ’£ ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ê°€ì ¸ì˜´
```

**ìˆ˜ì • ë°©ì•ˆ**:
```kotlin
val counts = elements.singleOrNull()
    ?: throw IllegalStateException(
        "Expected single aggregated result per window for product $key, " +
        "but got ${elements.count()} elements"
    )
```

**ê²€ì¦**:
```kotlin
@Test
fun `should throw exception when multiple elements in window`() {
    val processFunction = CTRResultWindowProcessFunction()
    val elements = listOf(
        EventCount(100, 50),
        EventCount(200, 100)  // ë‘ ê°œ!
    )

    assertThatThrownBy {
        processFunction.process("product-1", ..., elements, ...)
    }.isInstanceOf(IllegalStateException::class.java)
}
```

**ì˜ˆìƒ ì‹œê°„**: 2ì‹œê°„
**ì˜ì¡´ì„±**: ì—†ìŒ
**ì¤‘ìš”ë„**: P0 - ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥ì„±

---

### P0-2: Event.eventTimeMillisUtc() Silent Failure ìˆ˜ì •

**íŒŒì¼**: `flink-app/src/main/kotlin/com/example/ctr/domain/model/Event.kt`

**í˜„ì¬ ë¬¸ì œ**:
```kotlin
fun eventTimeMillisUtc(): Long = timestamp ?: 0L  // epoch 0 ë°˜í™˜
```

**ìˆ˜ì • ë°©ì•ˆ**:
```kotlin
fun eventTimeMillisUtc(): Long =
    timestamp ?: throw IllegalStateException(
        "Event timestamp is null for event: userId=$userId, productId=$productId, eventType=$eventType"
    )
```

**ëŒ€ì•ˆ (nullable ë°˜í™˜)**:
```kotlin
fun eventTimeMillisUtcOrNull(): Long? = timestamp

// ì‚¬ìš©ì²˜ì—ì„œ:
.filter { it.eventTimeMillisUtcOrNull() != null }
.assignTimestampsAndWatermarks(...)
```

**ê²€ì¦**:
```kotlin
@Test
fun `should throw exception for null timestamp`() {
    val event = Event(
        userId = "user1",
        productId = "prod1",
        eventType = "click",
        timestamp = null
    )

    assertThatThrownBy { event.eventTimeMillisUtc() }
        .isInstanceOf(IllegalStateException::class.java)
        .hasMessageContaining("timestamp is null")
}
```

**ì˜ˆìƒ ì‹œê°„**: 1ì‹œê°„
**ì˜ì¡´ì„±**: ì—†ìŒ
**ì¤‘ìš”ë„**: P0 - ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ

---

### P0-3: í…ŒìŠ¤íŠ¸ ì¸í”„ë¼ ë³µêµ¬

**í˜„ì¬ ë¬¸ì œ**:
```
> Task :test FAILED
> Could not start Gradle Test Executor 1
```

**ì›ì¸**: JUnit ë²„ì „ ë¯¸ëª…ì‹œ

**ìˆ˜ì • (build.gradle.kts)**:
```kotlin
dependencies {
    // ê¸°ì¡´
    testImplementation("org.junit.jupiter:junit-jupiter")

    // ìˆ˜ì •
    testImplementation("org.junit.jupiter:junit-jupiter:5.10.1")
    testImplementation("org.junit.jupiter:junit-jupiter-api:5.10.1")
    testRuntimeOnly("org.junit.jupiter:junit-jupiter-engine:5.10.1")

    // Flink test utilities
    testImplementation("org.apache.flink:flink-test-utils:$flinkVersion")
    testImplementation("org.apache.flink:flink-runtime:$flinkVersion:tests")
    testImplementation("org.apache.flink:flink-streaming-java:$flinkVersion:tests")
}

tasks.named<Test>("test") {
    useJUnitPlatform()

    // Test configuration
    maxHeapSize = "1G"
    testLogging {
        events("passed", "skipped", "failed")
        showStandardStreams = true
    }
}
```

**ê²€ì¦**:
```bash
cd flink-app
./gradlew clean test

# ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ í™•ì¸
> Task :test
EventCountTest > testInitialState() PASSED
EventCountTest > testIncrements() PASSED
...
BUILD SUCCESSFUL
```

**ì˜ˆìƒ ì‹œê°„**: 2ì‹œê°„
**ì˜ì¡´ì„±**: ì—†ìŒ
**ì¤‘ìš”ë„**: P0 - í…ŒìŠ¤íŠ¸ ë¶ˆê°€ëŠ¥

---

### P0-4: Gradle ë²„ì „ ê³ ì •

**í˜„ì¬ ë¬¸ì œ**: Gradle 9.x ì‚¬ìš© ì¤‘, FlinkëŠ” Gradle 8.x ê¶Œì¥

**ìˆ˜ì • (gradle/wrapper/gradle-wrapper.properties)**:
```properties
distributionBase=GRADLE_USER_HOME
distributionPath=wrapper/dists
distributionUrl=https\://services.gradle.org/distributions/gradle-8.5-bin.zip
zipStoreBase=GRADLE_USER_HOME
zipStorePath=wrapper/dists
```

**ê²€ì¦**:
```bash
./gradlew --version

------------------------------------------------------------
Gradle 8.5
------------------------------------------------------------
```

**ì˜ˆìƒ ì‹œê°„**: 30ë¶„
**ì˜ì¡´ì„±**: ì—†ìŒ
**ì¤‘ìš”ë„**: P0 - ë¹Œë“œ ì•ˆì •ì„±

---

### Phase 1 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] P0-1: CTRResultWindowProcessFunction ë²„ê·¸ ìˆ˜ì • ë° í…ŒìŠ¤íŠ¸ ì¶”ê°€
- [ ] P0-2: Event.eventTimeMillisUtc() null ì²˜ë¦¬ ê°œì„ 
- [ ] P0-3: í…ŒìŠ¤íŠ¸ ì¸í”„ë¼ ë³µêµ¬
- [ ] P0-4: Gradle ë²„ì „ ê³ ì •
- [ ] ëª¨ë“  ê¸°ì¡´ í…ŒìŠ¤íŠ¸ í†µê³¼ í™•ì¸
- [ ] ì½”ë“œ ë¦¬ë·° ì™„ë£Œ

**Phase 1 ì™„ë£Œ ì‹œ ë‹¬ì„±**:
- âœ… Critical ë²„ê·¸ 0ê°œ
- âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê°€ëŠ¥
- âœ… ë¹Œë“œ ì•ˆì •ì„± í™•ë³´

---

## Phase 2: Foundation (2-4ì£¼)

**ëª©í‘œ**: í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80%+, ê¸°ë³¸ ë³´ì•ˆ, CI/CD êµ¬ì¶•
**ì™„ë£Œ ì¡°ê±´**: ëª¨ë“  P1 ì´ìŠˆ í•´ê²°, ìë™í™”ëœ í…ŒìŠ¤íŠ¸/ë°°í¬

### P1-1: í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì¶”ê°€

#### Task 1.1: CtrJobPipelineBuilder í…ŒìŠ¤íŠ¸

**íŒŒì¼**: `flink-app/src/test/kotlin/com/example/ctr/infrastructure/flink/CtrJobPipelineBuilderTest.kt` (ìƒì„±)

**í…ŒìŠ¤íŠ¸ ë²”ìœ„**:
```kotlin
class CtrJobPipelineBuilderTest {

    @Test
    fun `should create Kafka source with correct configuration`() {
        val env = StreamExecutionEnvironment.getExecutionEnvironment()
        val builder = CtrJobPipelineBuilder(...)

        // Verify source configuration
        val transformation = builder.build(env).transformations[0]
        assertThat(transformation.name).isEqualTo("Kafka Source: impressions")
    }

    @Test
    fun `should apply watermark strategy with correct parameters`() {
        // Test watermark configuration
    }

    @Test
    fun `should create window with correct size`() {
        // Test window configuration
    }

    @Test
    fun `should assign UIDs for savepoint compatibility`() {
        // Test UID assignment
    }

    @Test
    fun `should create slot sharing groups correctly`() {
        // Test resource management
    }
}
```

**ì˜ˆìƒ ì‹œê°„**: 8ì‹œê°„
**ì¤‘ìš”ë„**: P1

---

#### Task 1.2: FlinkEnvironmentFactory í…ŒìŠ¤íŠ¸

**í…ŒìŠ¤íŠ¸ ë²”ìœ„**:
```kotlin
class FlinkEnvironmentFactoryTest {

    @Test
    fun `should configure checkpoint with correct settings`() {
        val properties = CtrJobProperties(
            name = "test-job",
            checkpointIntervalMs = 60000,
            checkpointTimeoutMs = 600000,
            minPauseBetweenCheckpointsMs = 500,
            parallelism = 2,
            restartAttemptsCount = 3,
            restartDelayMs = 10000
        )

        val factory = FlinkEnvironmentFactory(properties)
        val env = factory.create()

        val checkpointConfig = env.checkpointConfig
        assertThat(checkpointConfig.checkpointInterval).isEqualTo(60000)
        assertThat(checkpointConfig.checkpointingMode)
            .isEqualTo(CheckpointingMode.EXACTLY_ONCE)
    }

    @Test
    fun `should configure restart strategy`() {
        // Test restart configuration
    }
}
```

**ì˜ˆìƒ ì‹œê°„**: 4ì‹œê°„
**ì¤‘ìš”ë„**: P1

---

#### Task 1.3: KafkaSourceFactory í…ŒìŠ¤íŠ¸

**í…ŒìŠ¤íŠ¸ ë²”ìœ„**:
```kotlin
class KafkaSourceFactoryTest {

    @Test
    fun `should create source with correct bootstrap servers`() {
        val kafkaProps = KafkaProperties(
            bootstrapServers = "localhost:9092",
            groupId = "test-group"
        )

        val factory = KafkaSourceFactory(kafkaProps)
        val source = factory.createSource("test-topic", "group-1")

        // Verify source configuration
        // Note: Flink sourceëŠ” ì§ì ‘ ê²€ì¦ ì–´ë ¤ì›€,
        // ëŒ€ì‹  integration testì—ì„œ ê²€ì¦
    }
}
```

**ì˜ˆìƒ ì‹œê°„**: 2ì‹œê°„
**ì¤‘ìš”ë„**: P1

---

#### Task 1.4: ClickHouseSink í…ŒìŠ¤íŠ¸

**í…ŒìŠ¤íŠ¸ ë²”ìœ„**:
```kotlin
class ClickHouseSinkTest {

    private lateinit var testContainer: ClickHouseContainer

    @BeforeEach
    fun setup() {
        testContainer = ClickHouseContainer("clickhouse/clickhouse-server:23.8")
            .withInitScript("init-schema.sql")
        testContainer.start()
    }

    @Test
    fun `should insert CTR results to ClickHouse`() {
        val properties = ClickHouseProperties(
            url = testContainer.jdbcUrl,
            batchSize = 10,
            batchIntervalMs = 100,
            maxRetries = 3
        )

        val sink = ClickHouseSink(properties)
        val result = CTRResult(
            productId = "prod-1",
            impressions = 100,
            clicks = 50,
            ctr = 0.5,
            windowStart = Instant.now(),
            windowEnd = Instant.now().plusSeconds(10)
        )

        // Execute sink
        val env = StreamExecutionEnvironment.getExecutionEnvironment()
        env.fromElements(result)
            .addSink(sink.createSink())
        env.execute()

        // Verify data in ClickHouse
        val rs = testContainer.createConnection("").use { conn ->
            conn.createStatement().executeQuery(
                "SELECT * FROM ctr_results WHERE product_id = 'prod-1'"
            )
        }

        assertThat(rs.next()).isTrue()
        assertThat(rs.getDouble("ctr")).isEqualTo(0.5)
    }

    @AfterEach
    fun teardown() {
        testContainer.stop()
    }
}
```

**ì˜ì¡´ì„± ì¶”ê°€ (build.gradle.kts)**:
```kotlin
testImplementation("org.testcontainers:testcontainers:1.19.3")
testImplementation("org.testcontainers:clickhouse:1.19.3")
testImplementation("org.testcontainers:junit-jupiter:1.19.3")
```

**ì˜ˆìƒ ì‹œê°„**: 6ì‹œê°„
**ì¤‘ìš”ë„**: P1

---

### P1-2: Integration Test ì¶”ê°€

**íŒŒì¼**: `flink-app/src/test/kotlin/com/example/ctr/integration/CtrPipelineIntegrationTest.kt` (ìƒì„±)

```kotlin
class CtrPipelineIntegrationTest {

    companion object {
        @Container
        private val kafka = KafkaContainer(
            DockerImageName.parse("confluentinc/cp-kafka:7.4.0")
        ).withKraft()

        @Container
        private val clickhouse = ClickHouseContainer(
            "clickhouse/clickhouse-server:23.8"
        )
    }

    @Test
    fun `should process events end-to-end`() {
        // Given: Test events
        val impressions = listOf(
            """{"userId":"user1","productId":"prod1","eventType":"impression","timestamp":${now()}}""",
            """{"userId":"user2","productId":"prod1","eventType":"impression","timestamp":${now()}}"""
        )
        val clicks = listOf(
            """{"userId":"user1","productId":"prod1","eventType":"click","timestamp":${now()}}"""
        )

        // When: Produce to Kafka
        produceToKafka(kafka, "impressions", impressions)
        produceToKafka(kafka, "clicks", clicks)

        // Build and execute pipeline
        val env = StreamExecutionEnvironment.getExecutionEnvironment()
        env.setParallelism(1)
        env.setRuntimeMode(RuntimeExecutionMode.STREAMING)

        val config = AppConfig(
            kafka = KafkaProperties(
                bootstrapServers = kafka.bootstrapServers,
                groupId = "test-group"
            ),
            clickhouse = ClickHouseProperties(
                url = clickhouse.jdbcUrl
            ),
            ctr = CtrProperties(
                job = CtrJobProperties(...)
            )
        )

        val pipelineBuilder = CtrJobPipelineBuilder(...)
        pipelineBuilder.build(env)

        // Execute with timeout
        val jobExecutionResult = env.executeAsync("test-job")

        // Wait for processing
        Thread.sleep(15000)  // Wait for window to close

        jobExecutionResult.cancel()

        // Then: Verify results in ClickHouse
        clickhouse.createConnection("").use { conn ->
            val rs = conn.createStatement().executeQuery(
                "SELECT * FROM ctr_results WHERE product_id = 'prod1'"
            )

            assertThat(rs.next()).isTrue()
            assertThat(rs.getLong("impressions")).isEqualTo(2)
            assertThat(rs.getLong("clicks")).isEqualTo(1)
            assertThat(rs.getDouble("ctr")).isEqualTo(0.5)
        }
    }

    private fun produceToKafka(kafka: KafkaContainer, topic: String, messages: List<String>) {
        val props = Properties().apply {
            put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka.bootstrapServers)
            put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer::class.java)
            put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer::class.java)
        }

        KafkaProducer<String, String>(props).use { producer ->
            messages.forEach { message ->
                producer.send(ProducerRecord(topic, message)).get()
            }
        }
    }
}
```

**ì˜ˆìƒ ì‹œê°„**: 12ì‹œê°„
**ì¤‘ìš”ë„**: P1

---

### P1-3: ë³´ì•ˆ ê¸°ë³¸ ì„¤ì •

#### Task 3.1: Kafka SASL ì¸ì¦ ì¶”ê°€

**íŒŒì¼**: `flink-app/src/main/resources/application.yml`

```yaml
kafka:
  bootstrap-servers: "${KAFKA_BOOTSTRAP_SERVERS}"
  security:
    protocol: "SASL_SSL"
    sasl:
      mechanism: "PLAIN"
      jaas-config: |
        org.apache.kafka.common.security.plain.PlainLoginModule required
        username="${KAFKA_USERNAME}"
        password="${KAFKA_PASSWORD}";
  ssl:
    truststore-location: "${KAFKA_TRUSTSTORE_LOCATION:/etc/kafka/truststore.jks}"
    truststore-password: "${KAFKA_TRUSTSTORE_PASSWORD}"
```

**ì½”ë“œ ìˆ˜ì • (KafkaSourceFactory.kt)**:
```kotlin
fun createSource(topic: String, groupId: String): KafkaSource<Event> {
    val sourceBuilder = KafkaSource.builder<Event>()
        .setBootstrapServers(properties.bootstrapServers)
        .setTopics(topic)
        .setGroupId(groupId)
        .setDeserializer(EventDeserializationSchema())
        .setStartingOffsets(OffsetsInitializer.latest())

    // Add security configuration
    properties.security?.let { security ->
        val kafkaProps = Properties()
        kafkaProps["security.protocol"] = security.protocol

        security.sasl?.let { sasl ->
            kafkaProps["sasl.mechanism"] = sasl.mechanism
            kafkaProps["sasl.jaas.config"] = sasl.jaasConfig
        }

        security.ssl?.let { ssl ->
            kafkaProps["ssl.truststore.location"] = ssl.truststoreLocation
            kafkaProps["ssl.truststore.password"] = ssl.truststorePassword
        }

        sourceBuilder.setProperties(kafkaProps)
    }

    return sourceBuilder.build()
}
```

**ìƒˆ Properties í´ë˜ìŠ¤**:
```kotlin
data class KafkaSecurityProperties(
    val protocol: String,
    val sasl: SaslProperties?,
    val ssl: SslProperties?
)

data class SaslProperties(
    val mechanism: String,
    val jaasConfig: String
)

data class SslProperties(
    val truststoreLocation: String,
    val truststorePassword: String
)
```

**ì˜ˆìƒ ì‹œê°„**: 6ì‹œê°„
**ì¤‘ìš”ë„**: P1

---

#### Task 3.2: ClickHouse ì¸ì¦ ì¶”ê°€

**ìˆ˜ì • (application.yml)**:
```yaml
clickhouse:
  url: "jdbc:clickhouse://${CLICKHOUSE_HOST:clickhouse}:${CLICKHOUSE_PORT:8123}/${CLICKHOUSE_DB:default}"
  username: "${CLICKHOUSE_USERNAME:default}"
  password: "${CLICKHOUSE_PASSWORD}"
  ssl:
    enabled: ${CLICKHOUSE_SSL_ENABLED:false}
```

**ìˆ˜ì • (ClickHouseSink.kt)**:
```kotlin
fun createSink(): SinkFunction<CTRResult> {
    val connectionOptions = JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
        .withUrl(properties.url)
        .withDriverName("com.clickhouse.jdbc.ClickHouseDriver")
        .apply {
            properties.username?.let { withUsername(it) }
            properties.password?.let { withPassword(it) }
        }
        .build()

    // ... rest of the code
}
```

**ì˜ˆìƒ ì‹œê°„**: 2ì‹œê°„
**ì¤‘ìš”ë„**: P1

---

#### Task 3.3: Secrets Management

**ì˜µì…˜ 1: Kubernetes Secrets (ê¶Œì¥)**

```yaml
# k8s/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: flink-ctr-secrets
type: Opaque
stringData:
  kafka-username: "flink-user"
  kafka-password: "secure-password"
  clickhouse-password: "clickhouse-pass"
```

**Deploymentì—ì„œ ì‚¬ìš©**:
```yaml
env:
  - name: KAFKA_USERNAME
    valueFrom:
      secretKeyRef:
        name: flink-ctr-secrets
        key: kafka-username
  - name: KAFKA_PASSWORD
    valueFrom:
      secretKeyRef:
        name: flink-ctr-secrets
        key: kafka-password
```

**ì˜µì…˜ 2: HashiCorp Vault (ê³ ê¸‰)**

**ì˜ˆìƒ ì‹œê°„**: 4ì‹œê°„ (K8s) / 8ì‹œê°„ (Vault)
**ì¤‘ìš”ë„**: P1

---

### P1-4: CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

#### Task 4.1: GitHub Actions ì›Œí¬í”Œë¡œìš°

**íŒŒì¼**: `.github/workflows/ci.yml` (ìƒì„±)

```yaml
name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up JDK 17
        uses: actions/setup-java@v4
        with:
          java-version: '17'
          distribution: 'temurin'
          cache: gradle

      - name: Grant execute permission for gradlew
        run: chmod +x flink-app/gradlew

      - name: Run tests
        run: |
          cd flink-app
          ./gradlew clean test --info

      - name: Generate test report
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Test Results
          path: flink-app/build/test-results/test/*.xml
          reporter: java-junit

      - name: Upload test coverage
        uses: codecov/codecov-action@v3
        with:
          files: flink-app/build/reports/jacoco/test/jacocoTestReport.xml

  build:
    needs: test
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Set up JDK 17
        uses: actions/setup-java@v4
        with:
          java-version: '17'
          distribution: 'temurin'
          cache: gradle

      - name: Build JAR
        run: |
          cd flink-app
          ./gradlew clean shadowJar -x test

      - name: Upload JAR artifact
        uses: actions/upload-artifact@v4
        with:
          name: flink-app-jar
          path: flink-app/build/libs/ctr-calculator-*.jar

  docker:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v4

      - name: Download JAR
        uses: actions/download-artifact@v4
        with:
          name: flink-app-jar
          path: flink-app/build/libs/

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/flink-ctr-app:latest
            ${{ secrets.DOCKER_USERNAME }}/flink-ctr-app:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
```

**ì˜ˆìƒ ì‹œê°„**: 4ì‹œê°„
**ì¤‘ìš”ë„**: P1

---

#### Task 4.2: JaCoCo í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€

**build.gradle.kts ì¶”ê°€**:
```kotlin
plugins {
    kotlin("jvm") version "1.9.22"
    id("com.github.johnrengelman.shadow") version "8.1.1"
    id("jacoco")  // ì¶”ê°€
}

jacoco {
    toolVersion = "0.8.11"
}

tasks.test {
    finalizedBy(tasks.jacocoTestReport)
}

tasks.jacocoTestReport {
    dependsOn(tasks.test)

    reports {
        xml.required.set(true)
        html.required.set(true)
        csv.required.set(false)
    }

    classDirectories.setFrom(
        files(classDirectories.files.map {
            fileTree(it) {
                exclude(
                    "**/config/**",
                    "**/CtrApplication*",
                    "**/deserializer/**"
                )
            }
        })
    )
}

tasks.jacocoTestCoverageVerification {
    violationRules {
        rule {
            limit {
                minimum = "0.80".toBigDecimal()  // 80% ìµœì†Œ ì»¤ë²„ë¦¬ì§€
            }
        }
    }
}

tasks.check {
    dependsOn(tasks.jacocoTestCoverageVerification)
}
```

**ì˜ˆìƒ ì‹œê°„**: 2ì‹œê°„
**ì¤‘ìš”ë„**: P1

---

### P1-5: ì„¤ì • íŒŒë¼ë¯¸í„°í™”

#### Task 5.1: í•˜ë“œì½”ë”©ëœ ê°’ì„ ì„¤ì •ìœ¼ë¡œ ì´ë™

**ìˆ˜ì • ì „ (CtrJobPipelineBuilder.kt)**:
```kotlin
.window(TumblingEventTimeWindows.of(Time.seconds(10)))  // í•˜ë“œì½”ë”©
.assignTimestampsAndWatermarks(
    WatermarkStrategy
        .forBoundedOutOfOrderness<Event>(Duration.ofSeconds(5))  // í•˜ë“œì½”ë”©
)
.allowedLateness(Time.seconds(5))  // í•˜ë“œì½”ë”©
```

**ìƒˆ Properties í´ë˜ìŠ¤**:
```kotlin
data class CtrJobProperties(
    // ê¸°ì¡´ ì†ì„±ë“¤
    @field:NotBlank val name: String,
    @field:Positive val parallelism: Int,
    @field:Positive val checkpointIntervalMs: Long,
    // ... ê¸°íƒ€

    // ìƒˆë¡œ ì¶”ê°€
    @field:Positive
    val windowSizeSeconds: Long = 10,

    @field:Positive
    val watermarkMaxOutOfOrderSeconds: Long = 5,

    @field:Positive
    val allowedLatenessSeconds: Long = 5,

    @field:Positive
    val idleSourceTimeoutSeconds: Long = 30
)
```

**ìˆ˜ì • í›„ (CtrJobPipelineBuilder.kt)**:
```kotlin
private fun buildAggregation(
    stream: SingleOutputStreamOperator<Event>,
    aggregator: EventCountAggregator,
    windowFunction: CTRResultWindowProcessFunction,
    uidPrefix: String
): SingleOutputStreamOperator<CTRResult> {
    return stream
        .assignTimestampsAndWatermarks(
            WatermarkStrategy
                .forBoundedOutOfOrderness<Event>(
                    Duration.ofSeconds(properties.watermarkMaxOutOfOrderSeconds)
                )
                .withIdlenessTimeout(
                    Duration.ofSeconds(properties.idleSourceTimeoutSeconds)
                )
        )
        .uid("$uidPrefix-watermarks")
        .name("Assign Watermarks: $uidPrefix")
        .keyBy(Event::productId)
        .window(
            TumblingEventTimeWindows.of(
                Time.seconds(properties.windowSizeSeconds)
            )
        )
        .allowedLateness(
            Time.seconds(properties.allowedLatenessSeconds)
        )
        .aggregate(aggregator, windowFunction)
        .uid("$uidPrefix-aggregate")
        .name("Aggregate: $uidPrefix")
}
```

**application.yml ì—…ë°ì´íŠ¸**:
```yaml
ctr:
  job:
    name: "ctr-calculator"
    parallelism: 2
    checkpoint-interval-ms: 60000
    # ìƒˆë¡œ ì¶”ê°€
    window-size-seconds: 10
    watermark-max-out-of-order-seconds: 5
    allowed-lateness-seconds: 5
    idle-source-timeout-seconds: 30
```

**ì˜ˆìƒ ì‹œê°„**: 3ì‹œê°„
**ì¤‘ìš”ë„**: P1

---

#### Task 5.2: Kafka Offset ì´ˆê¸°í™” ì „ëµ ì„¤ì •

**Properties ì¶”ê°€**:
```kotlin
data class KafkaProperties(
    @field:NotBlank val bootstrapServers: String,
    @field:NotBlank val groupId: String,
    val security: KafkaSecurityProperties? = null,

    // ìƒˆë¡œ ì¶”ê°€
    val offsetResetStrategy: OffsetResetStrategy = OffsetResetStrategy.LATEST
)

enum class OffsetResetStrategy {
    EARLIEST,
    LATEST,
    TIMESTAMP
}
```

**KafkaSourceFactory ìˆ˜ì •**:
```kotlin
fun createSource(topic: String, groupId: String): KafkaSource<Event> {
    val offsetsInitializer = when (properties.offsetResetStrategy) {
        OffsetResetStrategy.EARLIEST -> OffsetsInitializer.earliest()
        OffsetResetStrategy.LATEST -> OffsetsInitializer.latest()
        OffsetResetStrategy.TIMESTAMP ->
            OffsetsInitializer.timestamp(properties.offsetTimestamp ?: 0L)
    }

    return KafkaSource.builder<Event>()
        .setBootstrapServers(properties.bootstrapServers)
        .setTopics(topic)
        .setGroupId(groupId)
        .setStartingOffsets(offsetsInitializer)
        .setDeserializer(EventDeserializationSchema())
        .build()
}
```

**ì˜ˆìƒ ì‹œê°„**: 2ì‹œê°„
**ì¤‘ìš”ë„**: P2

---

### P1-6: Configuration Validation ê°•ì œ

**AppConfig.kt ìˆ˜ì •**:
```kotlin
object AppConfig {
    private val logger = LoggerFactory.getLogger(AppConfig::class.java)
    private val validator: Validator = Validation.buildDefaultValidatorFactory().validator

    fun load(): AppConfig {
        val yamlFile = findConfigFile()
        logger.info("Loading configuration from: ${yamlFile.absolutePath}")

        val config = try {
            mapper.readValue(yamlFile, AppConfig::class.java)
        } catch (e: Exception) {
            throw IllegalStateException("Failed to load configuration from $yamlFile", e)
        }

        // Validation ê°•ì œ
        validateConfiguration(config)

        logger.info("Configuration loaded successfully")
        return config
    }

    private fun validateConfiguration(config: AppConfig) {
        val violations = validator.validate(config)

        if (violations.isNotEmpty()) {
            val errors = violations.joinToString("\n") { violation ->
                "${violation.propertyPath}: ${violation.message}"
            }
            throw IllegalStateException(
                "Configuration validation failed:\n$errors"
            )
        }

        // Custom validation
        require(config.ctr.job.parallelism > 0) {
            "Parallelism must be positive, got: ${config.ctr.job.parallelism}"
        }
        require(config.ctr.job.windowSizeSeconds > 0) {
            "Window size must be positive, got: ${config.ctr.job.windowSizeSeconds}"
        }
        // ... ì¶”ê°€ ê²€ì¦
    }
}
```

**í…ŒìŠ¤íŠ¸**:
```kotlin
class AppConfigTest {

    @Test
    fun `should throw exception for invalid parallelism`() {
        // Given: Invalid config file
        val invalidYaml = """
            ctr:
              job:
                parallelism: -1
        """.trimIndent()

        File("test-invalid.yml").writeText(invalidYaml)

        // When/Then
        assertThatThrownBy {
            AppConfig.load()
        }
            .isInstanceOf(IllegalStateException::class.java)
            .hasMessageContaining("Parallelism must be positive")
    }
}
```

**ì˜ˆìƒ ì‹œê°„**: 4ì‹œê°„
**ì¤‘ìš”ë„**: P1

---

### Phase 2 ì²´í¬ë¦¬ìŠ¤íŠ¸

#### í…ŒìŠ¤íŠ¸
- [ ] CtrJobPipelineBuilder ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (5+ test cases)
- [ ] FlinkEnvironmentFactory í…ŒìŠ¤íŠ¸ (3+ test cases)
- [ ] KafkaSourceFactory í…ŒìŠ¤íŠ¸
- [ ] ClickHouseSink í…ŒìŠ¤íŠ¸ (Testcontainers)
- [ ] Integration test (end-to-end)
- [ ] í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80% ë‹¬ì„±
- [ ] JaCoCo ë¦¬í¬íŠ¸ ìƒì„±

#### ë³´ì•ˆ
- [ ] Kafka SASL ì¸ì¦ êµ¬í˜„
- [ ] ClickHouse ì¸ì¦ êµ¬í˜„
- [ ] Secrets management (K8s Secrets or Vault)
- [ ] SSL/TLS ì„¤ì • (ì„ íƒì‚¬í•­)

#### CI/CD
- [ ] GitHub Actions ì›Œí¬í”Œë¡œìš° êµ¬ì„±
- [ ] ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- [ ] Docker ì´ë¯¸ì§€ ë¹Œë“œ ìë™í™”
- [ ] Test coverage reporting

#### ì„¤ì •
- [ ] í•˜ë“œì½”ë”© ê°’ ì œê±° (window, watermark, lateness)
- [ ] Kafka offset ì „ëµ ì„¤ì • ê°€ëŠ¥
- [ ] ClickHouse batch ì„¤ì • ê°€ëŠ¥
- [ ] Configuration validation ê°•ì œ

**Phase 2 ì™„ë£Œ ì‹œ ë‹¬ì„±**:
- âœ… í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80%+
- âœ… ê¸°ë³¸ ë³´ì•ˆ êµ¬í˜„
- âœ… CI/CD íŒŒì´í”„ë¼ì¸
- âœ… ì„¤ì • íŒŒë¼ë¯¸í„°í™”

---

## Phase 3: Production Ready (1-2ê°œì›”)

**ëª©í‘œ**: ëª¨ë‹ˆí„°ë§, ë¡œê¹…, ìš´ì˜ ë„êµ¬ êµ¬ì¶•
**ì™„ë£Œ ì¡°ê±´**: í”„ë¡œë•ì…˜ ë°°í¬ ê°€ëŠ¥ ìˆ˜ì¤€

### P2-1: Custom Metrics ì¶”ê°€

#### Task 1.1: Flink Metrics êµ¬í˜„

**ìƒˆ íŒŒì¼**: `flink-app/src/main/kotlin/com/example/ctr/infrastructure/flink/metrics/CtrMetrics.kt`

```kotlin
class CtrMetrics(runtimeContext: RuntimeContext) {
    private val metricGroup = runtimeContext.metricGroup

    // Counters
    val eventsProcessed: Counter = metricGroup.counter("events_processed")
    val eventsInvalid: Counter = metricGroup.counter("events_invalid")
    val clicksTotal: Counter = metricGroup.counter("clicks_total")
    val impressionsTotal: Counter = metricGroup.counter("impressions_total")

    // Gauges
    @Volatile private var currentCtr: Double = 0.0
    val ctrGauge: Gauge<Double> = metricGroup.gauge("current_ctr") { currentCtr }

    @Volatile private var activeProducts: Long = 0
    val activeProductsGauge: Gauge<Long> =
        metricGroup.gauge("active_products") { activeProducts }

    // Histograms
    val ctrDistribution: Histogram = metricGroup.histogram(
        "ctr_distribution",
        DescriptiveStatisticsHistogram(1000)
    )

    val windowProcessingTime: Histogram = metricGroup.histogram(
        "window_processing_time_ms",
        DescriptiveStatisticsHistogram(1000)
    )

    fun recordCtr(ctr: Double) {
        currentCtr = ctr
        ctrDistribution.update((ctr * 1000).toLong())  // Convert to basis points
    }

    fun recordWindowProcessingTime(durationMs: Long) {
        windowProcessingTime.update(durationMs)
    }

    fun updateActiveProducts(count: Long) {
        activeProducts = count
    }
}
```

**CTRResultWindowProcessFunctionì— ì ìš©**:
```kotlin
class CTRResultWindowProcessFunction : ProcessWindowFunction<EventCount, CTRResult, String, TimeWindow>() {

    @Transient
    private lateinit var metrics: CtrMetrics

    override fun open(parameters: Configuration) {
        super.open(parameters)
        metrics = CtrMetrics(runtimeContext)
    }

    override fun process(
        key: String,
        context: Context,
        elements: Iterable<EventCount>,
        out: Collector<CTRResult>
    ) {
        val startTime = System.currentTimeMillis()

        val counts = elements.singleOrNull()
            ?: throw IllegalStateException("Expected single aggregated result")

        val result = CTRResult.from(
            productId = key,
            counts = counts,
            windowStart = Instant.ofEpochMilli(context.window().start),
            windowEnd = Instant.ofEpochMilli(context.window().end)
        )

        // Record metrics
        metrics.recordCtr(result.ctr)
        metrics.recordWindowProcessingTime(System.currentTimeMillis() - startTime)

        out.collect(result)
    }
}
```

**EventDeserializationSchemaì— ì ìš©**:
```kotlin
class EventDeserializationSchema : DeserializationSchema<Event> {

    @Transient
    private lateinit var metrics: CtrMetrics

    override fun open(context: DeserializationSchema.InitializationContext) {
        super.open(context)
        metrics = CtrMetrics(context.metricGroup)
    }

    override fun deserialize(message: ByteArray): Event? {
        return try {
            val event = objectMapper.readValue(message, Event::class.java)

            if (event.isValid()) {
                metrics.eventsProcessed.inc()

                when (event.eventType) {
                    "click" -> metrics.clicksTotal.inc()
                    "impression", "view" -> metrics.impressionsTotal.inc()
                }

                event
            } else {
                metrics.eventsInvalid.inc()
                logger.warn("Dropping invalid event: $event")
                null
            }
        } catch (e: Exception) {
            metrics.eventsInvalid.inc()
            logger.warn("Failed to deserialize event", e)
            null
        }
    }
}
```

**ì˜ˆìƒ ì‹œê°„**: 8ì‹œê°„
**ì¤‘ìš”ë„**: P2

---

#### Task 1.2: Prometheus Metrics Reporter ì„¤ì •

**flink-conf.yaml ì¶”ê°€** (docker-compose.yml ë˜ëŠ” K8s ConfigMap):
```yaml
metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter
metrics.reporter.prom.port: 9249
```

**Prometheus scrape config**:
```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'flink'
    static_configs:
      - targets: ['flink-jobmanager:9249', 'flink-taskmanager:9249']
    metrics_path: /metrics
```

**ì˜ˆìƒ ì‹œê°„**: 4ì‹œê°„
**ì¤‘ìš”ë„**: P2

---

### P2-2: Structured Logging

**ìƒˆ íŒŒì¼**: `flink-app/src/main/kotlin/com/example/ctr/infrastructure/logging/StructuredLogger.kt`

```kotlin
object StructuredLogger {
    private val objectMapper = ObjectMapper().apply {
        registerModule(JavaTimeModule())
    }

    fun info(
        logger: Logger,
        message: String,
        vararg fields: Pair<String, Any?>
    ) {
        val logEntry = buildLogEntry("INFO", message, fields.toMap())
        logger.info(objectMapper.writeValueAsString(logEntry))
    }

    fun warn(
        logger: Logger,
        message: String,
        throwable: Throwable? = null,
        vararg fields: Pair<String, Any?>
    ) {
        val logEntry = buildLogEntry("WARN", message, fields.toMap(), throwable)
        logger.warn(objectMapper.writeValueAsString(logEntry))
    }

    fun error(
        logger: Logger,
        message: String,
        throwable: Throwable,
        vararg fields: Pair<String, Any?>
    ) {
        val logEntry = buildLogEntry("ERROR", message, fields.toMap(), throwable)
        logger.error(objectMapper.writeValueAsString(logEntry))
    }

    private fun buildLogEntry(
        level: String,
        message: String,
        fields: Map<String, Any?>,
        throwable: Throwable? = null
    ): Map<String, Any?> {
        return mutableMapOf(
            "timestamp" to Instant.now().toString(),
            "level" to level,
            "message" to message,
            "thread" to Thread.currentThread().name
        ).apply {
            putAll(fields)
            throwable?.let {
                put("exception", mapOf(
                    "class" to it.javaClass.name,
                    "message" to it.message,
                    "stackTrace" to it.stackTraceToString()
                ))
            }
        }
    }
}
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```kotlin
class CTRResultWindowProcessFunction : ProcessWindowFunction<...> {

    override fun process(...) {
        StructuredLogger.info(
            logger,
            "Processing CTR window",
            "productId" to key,
            "windowStart" to context.window().start,
            "windowEnd" to context.window().end,
            "impressions" to counts.impressions,
            "clicks" to counts.clicks,
            "ctr" to result.ctr
        )
    }
}
```

**logback.xml ì„¤ì •**:
```xml
<configuration>
    <appender name="JSON" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <customFields>{"application":"flink-ctr-calculator"}</customFields>
        </encoder>
    </appender>

    <root level="INFO">
        <appender-ref ref="JSON"/>
    </root>
</configuration>
```

**ì˜ì¡´ì„± ì¶”ê°€**:
```kotlin
implementation("net.logstash.logback:logstash-logback-encoder:7.4")
```

**ì˜ˆìƒ ì‹œê°„**: 6ì‹œê°„
**ì¤‘ìš”ë„**: P2

---

### P2-3: Alerting êµ¬í˜„

**ìƒˆ íŒŒì¼**: `k8s/prometheus-rules.yaml`

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: flink-ctr-alerts
  labels:
    prometheus: kube-prometheus
spec:
  groups:
    - name: flink-ctr
      interval: 30s
      rules:
        # Job ìƒíƒœ ì•Œë¦¼
        - alert: FlinkJobDown
          expr: up{job="flink"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Flink job is down"
            description: "Flink job {{ $labels.instance }} has been down for more than 5 minutes"

        # Checkpoint ì‹¤íŒ¨ ì•Œë¦¼
        - alert: CheckpointFailureRate
          expr: rate(flink_jobmanager_job_numberOfFailedCheckpoints[5m]) > 0.1
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High checkpoint failure rate"
            description: "Checkpoint failure rate is {{ $value }} per second"

        # Backpressure ì•Œë¦¼
        - alert: HighBackpressure
          expr: flink_taskmanager_job_task_backPressuredTimeMsPerSecond > 500
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High backpressure detected"
            description: "Task {{ $labels.task_name }} is backpressured {{ $value }}ms/s"

        # CTR ì´ìƒ ì•Œë¦¼
        - alert: CTRAnomalyDetected
          expr: |
            (
              avg_over_time(current_ctr[1h])
              - avg_over_time(current_ctr[24h] offset 1d)
            ) / avg_over_time(current_ctr[24h] offset 1d) > 0.5
          for: 30m
          labels:
            severity: warning
          annotations:
            summary: "CTR anomaly detected"
            description: "CTR has changed by {{ $value }}% compared to yesterday"

        # ë°ì´í„° ì²˜ë¦¬ëŸ‰ ê¸‰ê° ì•Œë¦¼
        - alert: LowEventThroughput
          expr: rate(events_processed[5m]) < 100
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Low event processing rate"
            description: "Processing only {{ $value }} events/s (expected > 100)"

        # Invalid event ë¹„ìœ¨ ì•Œë¦¼
        - alert: HighInvalidEventRate
          expr: |
            rate(events_invalid[5m])
            / (rate(events_processed[5m]) + rate(events_invalid[5m]))
            > 0.1
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High invalid event rate"
            description: "{{ $value }}% of events are invalid"
```

**AlertManager ì„¤ì •** (`k8s/alertmanager-config.yaml`):
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m

    route:
      group_by: ['alertname', 'severity']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'slack'

      routes:
        - match:
            severity: critical
          receiver: 'pagerduty'
          continue: true

        - match:
            severity: warning
          receiver: 'slack'

    receivers:
      - name: 'slack'
        slack_configs:
          - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
            channel: '#flink-alerts'
            title: '{{ .CommonAnnotations.summary }}'
            text: '{{ .CommonAnnotations.description }}'

      - name: 'pagerduty'
        pagerduty_configs:
          - service_key: 'YOUR_PAGERDUTY_KEY'
```

**ì˜ˆìƒ ì‹œê°„**: 8ì‹œê°„
**ì¤‘ìš”ë„**: P2

---

### P2-4: Graceful Shutdown êµ¬í˜„

**CtrJobService.kt ìˆ˜ì •**:
```kotlin
class CtrJobService(
    private val environmentFactory: FlinkEnvironmentFactory,
    private val pipelineBuilder: CtrJobPipelineBuilder
) {
    private val logger = LoggerFactory.getLogger(CtrJobService::class.java)

    @Volatile
    private var env: StreamExecutionEnvironment? = null

    @Volatile
    private var jobExecutionResult: JobClient? = null

    @Volatile
    private var isShuttingDown = false

    fun execute() {
        try {
            env = environmentFactory.create()
            pipelineBuilder.build(env!!)

            // Register shutdown hook BEFORE starting job
            registerShutdownHook()

            logger.info("Starting Flink job...")
            jobExecutionResult = env!!.executeAsync(env!!.configuration.getString("ctr.job.name", "ctr-calculator"))

            // Wait for job completion
            jobExecutionResult?.jobExecutionResult?.get()

        } catch (e: Exception) {
            if (!isShuttingDown) {
                logger.error("Flink job execution failed", e)
                throw RuntimeException("Failed to execute Flink job", e)
            }
        }
    }

    private fun registerShutdownHook() {
        Runtime.getRuntime().addShutdownHook(Thread({
            logger.info("Shutdown signal received, initiating graceful shutdown...")
            isShuttingDown = true

            try {
                jobExecutionResult?.let { client ->
                    logger.info("Triggering savepoint before shutdown...")

                    // Trigger savepoint
                    val savepointPath = client.stopWithSavepoint(
                        false,  // don't drain
                        "/tmp/savepoints",
                        SavepointFormatType.CANONICAL
                    ).get(5, TimeUnit.MINUTES)

                    logger.info("Savepoint created at: $savepointPath")

                    // Cancel job gracefully
                    client.cancel().get(1, TimeUnit.MINUTES)
                    logger.info("Job cancelled successfully")
                }

                env?.close()
                logger.info("Flink environment closed successfully")

            } catch (e: TimeoutException) {
                logger.error("Shutdown timeout - forcing termination", e)
                jobExecutionResult?.cancel()
            } catch (e: Exception) {
                logger.error("Error during graceful shutdown", e)
            }
        }, "shutdown-hook"))
    }
}
```

**Kubernetes Deploymentì—ì„œ ì‚¬ìš©**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flink-jobmanager
spec:
  template:
    spec:
      containers:
        - name: jobmanager
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh", "-c", "sleep 15"]  # Grace period
          terminationGracePeriodSeconds: 60
```

**ì˜ˆìƒ ì‹œê°„**: 4ì‹œê°„
**ì¤‘ìš”ë„**: P2

---

### P2-5: State Backend ìµœì í™”

**FlinkEnvironmentFactory.kt ìˆ˜ì •**:
```kotlin
class FlinkEnvironmentFactory(private val properties: CtrJobProperties) {

    fun create(): StreamExecutionEnvironment {
        val env = StreamExecutionEnvironment.getExecutionEnvironment()

        // Basic configuration
        env.setParallelism(properties.parallelism)
        env.setRestartStrategy(...)

        // State backend configuration
        configureStateBackend(env)

        // Checkpoint configuration
        configureCheckpointing(env)

        return env
    }

    private fun configureStateBackend(env: StreamExecutionEnvironment) {
        when (properties.stateBackend) {
            StateBackendType.FILESYSTEM -> {
                // Default - already configured
                logger.info("Using FileSystem state backend")
            }

            StateBackendType.ROCKSDB -> {
                val rocksDB = EmbeddedRocksDBStateBackend(true)  // incremental checkpoints

                // Performance tuning
                rocksDB.setPredefinedOptions(PredefinedOptions.SPINNING_DISK_OPTIMIZED)

                // Custom RocksDB options
                val options = DefaultConfigurableOptionsFactory()
                options.setMaxBackgroundThreads(4)
                options.setMaxOpenFiles(-1)
                rocksDB.setRocksDBOptions(options)

                env.setStateBackend(rocksDB)
                logger.info("Using RocksDB state backend with incremental checkpoints")
            }
        }

        // State TTL for preventing unbounded state growth
        if (properties.stateTtlHours > 0) {
            logger.info("State TTL configured: ${properties.stateTtlHours} hours")
            // TTLì€ ê°œë³„ stateì—ì„œ ì„¤ì • (ë‹¤ìŒ phaseì—ì„œ êµ¬í˜„)
        }
    }
}

enum class StateBackendType {
    FILESYSTEM,
    ROCKSDB
}
```

**Properties ì¶”ê°€**:
```kotlin
data class CtrJobProperties(
    // ... ê¸°ì¡´ ì†ì„±ë“¤

    val stateBackend: StateBackendType = StateBackendType.FILESYSTEM,
    val stateTtlHours: Long = 0,  // 0 = disabled
    val checkpointStorage: String = "file:///tmp/flink-checkpoints"
)
```

**application.yml**:
```yaml
ctr:
  job:
    state-backend: ROCKSDB
    state-ttl-hours: 24
    checkpoint-storage: "s3://my-bucket/flink-checkpoints"  # For production
```

**ì˜ì¡´ì„± ì¶”ê°€**:
```kotlin
// build.gradle.kts
implementation("org.apache.flink:flink-statebackend-rocksdb:$flinkVersion")
```

**ì˜ˆìƒ ì‹œê°„**: 6ì‹œê°„
**ì¤‘ìš”ë„**: P2

---

### P2-6: Kubernetes ë°°í¬ ì¤€ë¹„

#### Task 6.1: Helm Chart ì‘ì„±

**ë””ë ‰í† ë¦¬ êµ¬ì¡°**:
```
k8s/
â”œâ”€â”€ helm/
â”‚   â””â”€â”€ flink-ctr/
â”‚       â”œâ”€â”€ Chart.yaml
â”‚       â”œâ”€â”€ values.yaml
â”‚       â”œâ”€â”€ templates/
â”‚       â”‚   â”œâ”€â”€ jobmanager-deployment.yaml
â”‚       â”‚   â”œâ”€â”€ taskmanager-deployment.yaml
â”‚       â”‚   â”œâ”€â”€ jobmanager-service.yaml
â”‚       â”‚   â”œâ”€â”€ configmap.yaml
â”‚       â”‚   â”œâ”€â”€ secrets.yaml
â”‚       â”‚   â””â”€â”€ servicemonitor.yaml
â”‚       â””â”€â”€ README.md
```

**Chart.yaml**:
```yaml
apiVersion: v2
name: flink-ctr
description: Real-time CTR calculation pipeline
type: application
version: 1.0.0
appVersion: "1.0.0"
```

**values.yaml**:
```yaml
image:
  repository: your-docker-repo/flink-ctr-app
  tag: latest
  pullPolicy: IfNotPresent

jobmanager:
  replicas: 1
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"

taskmanager:
  replicas: 2
  resources:
    requests:
      memory: "2Gi"
      cpu: "1000m"
    limits:
      memory: "4Gi"
      cpu: "2000m"
  taskSlots: 2

flink:
  parallelism: 2
  checkpointInterval: 60000
  checkpointStorage: "s3://your-bucket/checkpoints"
  savepointDirectory: "s3://your-bucket/savepoints"

  stateBackend: "rocksdb"
  stateTtlHours: 24

kafka:
  bootstrapServers: "kafka-broker-1:9092,kafka-broker-2:9092"
  groupId: "flink-ctr-consumer"
  security:
    enabled: true
    protocol: "SASL_SSL"

clickhouse:
  host: "clickhouse.default.svc.cluster.local"
  port: 8123
  database: "default"

monitoring:
  prometheus:
    enabled: true
    port: 9249

  metrics:
    reporters: "prom"

secrets:
  kafka:
    username: "flink-user"
    password: "changeme"
  clickhouse:
    password: "changeme"
```

**templates/jobmanager-deployment.yaml**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "flink-ctr.fullname" . }}-jobmanager
  labels:
    {{- include "flink-ctr.labels" . | nindent 4 }}
    component: jobmanager
spec:
  replicas: {{ .Values.jobmanager.replicas }}
  selector:
    matchLabels:
      {{- include "flink-ctr.selectorLabels" . | nindent 6 }}
      component: jobmanager
  template:
    metadata:
      labels:
        {{- include "flink-ctr.selectorLabels" . | nindent 8 }}
        component: jobmanager
    spec:
      containers:
        - name: jobmanager
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          args: ["jobmanager"]
          ports:
            - containerPort: 6123
              name: rpc
            - containerPort: 6124
              name: blob-server
            - containerPort: 8081
              name: webui
            - containerPort: 9249
              name: metrics
          env:
            - name: KAFKA_BOOTSTRAP_SERVERS
              value: {{ .Values.kafka.bootstrapServers }}
            - name: KAFKA_USERNAME
              valueFrom:
                secretKeyRef:
                  name: {{ include "flink-ctr.fullname" . }}-secrets
                  key: kafka-username
            - name: KAFKA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ include "flink-ctr.fullname" . }}-secrets
                  key: kafka-password
            - name: CLICKHOUSE_HOST
              value: {{ .Values.clickhouse.host }}
            - name: CLICKHOUSE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ include "flink-ctr.fullname" . }}-secrets
                  key: clickhouse-password
          resources:
            {{- toYaml .Values.jobmanager.resources | nindent 12 }}
          volumeMounts:
            - name: flink-config
              mountPath: /opt/flink/conf
      volumes:
        - name: flink-config
          configMap:
            name: {{ include "flink-ctr.fullname" . }}-config
```

**ì˜ˆìƒ ì‹œê°„**: 12ì‹œê°„
**ì¤‘ìš”ë„**: P2

---

### Phase 3 ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ëª¨ë‹ˆí„°ë§
- [ ] Custom Flink metrics êµ¬í˜„
- [ ] Prometheus metrics reporter ì„¤ì •
- [ ] Grafana dashboard ìƒì„±
- [ ] Alerting rules êµ¬ì„±
- [ ] AlertManager ì„¤ì •

#### ë¡œê¹…
- [ ] Structured logging êµ¬í˜„
- [ ] JSON log format
- [ ] Log aggregation (ELK/Loki)
- [ ] ì£¼ìš” ì´ë²¤íŠ¸ ë¡œê¹…

#### ìš´ì˜
- [ ] Graceful shutdown êµ¬í˜„
- [ ] State backend ìµœì í™” (RocksDB)
- [ ] Checkpoint storage (S3/GCS)
- [ ] Kubernetes Helm chart
- [ ] Horizontal Pod Autoscaler

**Phase 3 ì™„ë£Œ ì‹œ ë‹¬ì„±**:
- âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- âœ… ìë™ ì•Œë¦¼
- âœ… ì•ˆì •ì ì¸ shutdown
- âœ… K8s ë°°í¬ ê°€ëŠ¥

---

## Phase 4: Scale & Optimize (2-3ê°œì›”)

**ëª©í‘œ**: ëŒ€ê·œëª¨ íŠ¸ë˜í”½ ì²˜ë¦¬, ì„±ëŠ¥ ìµœì í™”
**ì™„ë£Œ ì¡°ê±´**: ì´ˆë‹¹ 100ë§Œ ì´ë²¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥

### P3-1: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

#### Task 1.1: ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë„êµ¬ êµ¬ì¶•

**ìƒˆ íŒŒì¼**: `performance-test/load-generator.py`

```python
#!/usr/bin/env python3
import asyncio
import json
import time
from datetime import datetime
from kafka import KafkaProducer
from faker import Faker
import random

fake = Faker()

class LoadGenerator:
    def __init__(self, bootstrap_servers, events_per_second):
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        self.events_per_second = events_per_second
        self.product_ids = [f"product-{i}" for i in range(1000)]

    def generate_event(self, event_type):
        return {
            "userId": fake.uuid4(),
            "productId": random.choice(self.product_ids),
            "eventType": event_type,
            "timestamp": int(datetime.now().timestamp() * 1000)
        }

    async def produce_events(self):
        interval = 1.0 / self.events_per_second

        while True:
            # 80% impressions, 20% clicks
            event_type = "impression" if random.random() < 0.8 else "click"
            topic = "impressions" if event_type == "impression" else "clicks"

            event = self.generate_event(event_type)
            self.producer.send(topic, value=event)

            await asyncio.sleep(interval)

    async def run(self, duration_seconds=None):
        tasks = [self.produce_events() for _ in range(10)]  # 10 concurrent producers

        if duration_seconds:
            await asyncio.wait_for(
                asyncio.gather(*tasks),
                timeout=duration_seconds
            )
        else:
            await asyncio.gather(*tasks)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--bootstrap-servers", default="localhost:9092")
    parser.add_argument("--events-per-second", type=int, default=1000)
    parser.add_argument("--duration", type=int, help="Duration in seconds")

    args = parser.parse_args()

    generator = LoadGenerator(args.bootstrap_servers, args.events_per_second)

    asyncio.run(generator.run(args.duration))
```

**ì‹¤í–‰**:
```bash
# ì´ˆë‹¹ 1,000 ì´ë²¤íŠ¸
python load-generator.py --events-per-second 1000 --duration 300

# ì´ˆë‹¹ 10,000 ì´ë²¤íŠ¸
python load-generator.py --events-per-second 10000 --duration 300

# ì´ˆë‹¹ 100,000 ì´ë²¤íŠ¸
python load-generator.py --events-per-second 100000 --duration 300
```

**ì˜ˆìƒ ì‹œê°„**: 8ì‹œê°„
**ì¤‘ìš”ë„**: P3

---

#### Task 1.2: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

**Grafana Dashboard JSON** (`k8s/grafana-dashboard-performance.json`):

ì£¼ìš” ë©”íŠ¸ë¦­:
- Events/second (input rate)
- Records/second (output rate)
- End-to-end latency (p50, p95, p99)
- Checkpoint duration
- State size
- CPU/Memory usage
- Backpressure

**ì˜ˆìƒ ì‹œê°„**: 4ì‹œê°„
**ì¤‘ìš”ë„**: P3

---

### P3-2: Operator Chaining ìµœì í™”

**Task**: ì´ë¯¸ ì‘ì„±ëœ ë¬¸ì„œ (`docs/OPERATOR_CHAINING_*.md`) ê¸°ë°˜ìœ¼ë¡œ ìµœì í™” ì ìš©

**í˜„ì¬ chaining ìƒíƒœ ë¶„ì„**:
```bash
# Flink Web UIì—ì„œ í™•ì¸
# ë˜ëŠ” REST API:
curl http://localhost:8081/jobs/{job-id}/plan
```

**ìµœì í™” ì ìš© (CtrJobPipelineBuilder.kt)**:
```kotlin
// í•„ìš”í•œ ê²½ìš°ì—ë§Œ chaining ì¤‘ë‹¨
.filter(Event::isValid)
.disableChaining()  // í•„í„° í›„ chaining ì¤‘ë‹¨
.keyBy(Event::productId)
```

**ì˜ˆìƒ ì‹œê°„**: 6ì‹œê°„
**ì¤‘ìš”ë„**: P3

---

### P3-3: Network Buffer íŠœë‹

**flink-conf.yaml íŠœë‹**:
```yaml
taskmanager.network.memory.fraction: 0.2
taskmanager.network.memory.min: 256mb
taskmanager.network.memory.max: 1gb

taskmanager.network.numberOfBuffers: 8192
taskmanager.network.bufferSize: 32768

# Backpressure ê´€ë ¨
taskmanager.network.request-backoff.initial: 100
taskmanager.network.request-backoff.max: 10000
```

**ì˜ˆìƒ ì‹œê°„**: 4ì‹œê°„
**ì¤‘ìš”ë„**: P3

---

### P3-4: State TTL êµ¬í˜„

**EventCountAggregatorì— TTL ì¶”ê°€**:
```kotlin
class EventCountAggregator : AggregateFunction<Event, EventCount, EventCount> {

    private lateinit var stateDescriptor: AggregatingStateDescriptor<Event, EventCount, EventCount>

    fun createStateDescriptor(ttlHours: Long): AggregatingStateDescriptor<Event, EventCount, EventCount> {
        stateDescriptor = AggregatingStateDescriptor(
            "event-count-agg",
            this,
            EventCount::class.java
        )

        if (ttlHours > 0) {
            val ttlConfig = StateTtlConfig
                .newBuilder(Time.hours(ttlHours))
                .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)
                .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
                .cleanupFullSnapshot()
                .build()

            stateDescriptor.enableTimeToLive(ttlConfig)
        }

        return stateDescriptor
    }

    // ... ê¸°ì¡´ ë©”ì„œë“œë“¤
}
```

**ì˜ˆìƒ ì‹œê°„**: 4ì‹œê°„
**ì¤‘ìš”ë„**: P3

---

### Phase 4 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë„êµ¬ êµ¬ì¶•
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (1K, 10K, 100K, 1M events/s)
- [ ] Operator chaining ìµœì í™”
- [ ] Network buffer íŠœë‹
- [ ] State TTL êµ¬í˜„
- [ ] Incremental checkpoint ê²€ì¦
- [ ] Auto-scaling êµ¬ì„± (HPA)
- [ ] Cost optimization

**Phase 4 ì™„ë£Œ ì‹œ ë‹¬ì„±**:
- âœ… ì´ˆë‹¹ 100ë§Œ ì´ë²¤íŠ¸ ì²˜ë¦¬
- âœ… p99 latency < 5ì´ˆ
- âœ… Checkpoint ì‹œê°„ < 30ì´ˆ
- âœ… ë¦¬ì†ŒìŠ¤ ìµœì í™”

---

## Phase 5: Excellence (ì§€ì†ì )

**ëª©í‘œ**: ë°ì´í„° ê±°ë²„ë„ŒìŠ¤, ê³ ê¸‰ ê¸°ëŠ¥
**ì™„ë£Œ ì¡°ê±´**: ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ í’ˆì§ˆ

### P3-1: Data Quality Framework

```kotlin
class DataQualityValidator {
    fun validate(event: Event): ValidationResult {
        val violations = mutableListOf<String>()

        // Schema validation
        if (event.userId.isNullOrBlank()) violations.add("userId is required")
        if (event.productId.isNullOrBlank()) violations.add("productId is required")

        // Business rules
        if (event.timestamp != null && event.timestamp > System.currentTimeMillis()) {
            violations.add("timestamp cannot be in the future")
        }

        // Data quality metrics
        recordValidationMetrics(violations)

        return if (violations.isEmpty()) {
            ValidationResult.Valid
        } else {
            ValidationResult.Invalid(violations)
        }
    }
}
```

---

### P3-2: Schema Registry í†µí•©

**Avro Schema ì •ì˜**:
```json
{
  "type": "record",
  "name": "Event",
  "namespace": "com.example.ctr.avro",
  "fields": [
    {"name": "userId", "type": "string"},
    {"name": "productId", "type": "string"},
    {"name": "eventType", "type": {"type": "enum", "name": "EventType", "symbols": ["CLICK", "IMPRESSION"]}},
    {"name": "timestamp", "type": "long", "logicalType": "timestamp-millis"}
  ]
}
```

---

### P3-3: Data Lineage Tracking

**OpenLineage í†µí•©**:
```kotlin
class LineageTracker {
    fun recordDataset(name: String, namespace: String) {
        // Record dataset in OpenLineage
    }

    fun recordJob(jobName: String, inputs: List<Dataset>, outputs: List<Dataset>) {
        // Record job lineage
    }
}
```

---

### Phase 5 ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Data quality framework
- [ ] Schema registry (Confluent/AWS Glue)
- [ ] Data lineage tracking
- [ ] PII data masking
- [ ] GDPR compliance (data deletion)
- [ ] Audit logging
- [ ] Multi-tenancy support
- [ ] Advanced windowing (session, sliding)

---

## Production Readiness Checklist

### Infrastructure
- [ ] Kubernetes cluster ready
- [ ] S3/GCS for checkpoint storage
- [ ] Kafka cluster (3+ brokers)
- [ ] ClickHouse cluster (2+ replicas)
- [ ] Prometheus + Grafana
- [ ] ELK/Loki for logs

### Security
- [ ] Kafka SASL authentication
- [ ] ClickHouse authentication
- [ ] SSL/TLS encryption
- [ ] Network policies (K8s)
- [ ] Secrets management (Vault/K8s Secrets)
- [ ] RBAC configured

### Monitoring
- [ ] Custom metrics implemented
- [ ] Dashboards created
- [ ] Alerts configured
- [ ] On-call rotation setup
- [ ] Runbook documentation

### Testing
- [ ] Unit test coverage > 80%
- [ ] Integration tests pass
- [ ] Load testing completed
- [ ] Chaos engineering (optional)
- [ ] Disaster recovery tested

### Documentation
- [ ] Architecture diagrams
- [ ] API documentation
- [ ] Runbook
- [ ] Troubleshooting guide
- [ ] ADRs (Architecture Decision Records)

### CI/CD
- [ ] Automated testing
- [ ] Docker image build
- [ ] Helm chart deployment
- [ ] Blue-green deployment
- [ ] Rollback procedure

---

## ì˜ˆìƒ íƒ€ì„ë¼ì¸

| Phase | Duration | FTE | Completion |
|-------|----------|-----|------------|
| Phase 1: Critical Fixes | 1-2ì£¼ | 1.0 | 2ì£¼ |
| Phase 2: Foundation | 2-4ì£¼ | 1.0 | 6ì£¼ |
| Phase 3: Production Ready | 1-2ê°œì›” | 1.0 | 3ê°œì›” |
| Phase 4: Scale & Optimize | 2-3ê°œì›” | 1.0 | 6ê°œì›” |
| Phase 5: Excellence | ì§€ì†ì  | 0.5 | Ongoing |

**Total: 6ê°œì›” full-time work**

---

## ì„±ê³µ ê¸°ì¤€

### ê¸°ìˆ ì  ì„±ê³µ
- âœ… í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80%+
- âœ… ì´ˆë‹¹ 100ë§Œ ì´ë²¤íŠ¸ ì²˜ë¦¬
- âœ… p99 latency < 5ì´ˆ
- âœ… 99.9% uptime
- âœ… Zero data loss

### ìš´ì˜ì  ì„±ê³µ
- âœ… MTTR (Mean Time To Recovery) < 15ë¶„
- âœ… Deployment frequency: daily
- âœ… Change failure rate < 5%
- âœ… On-call incidents < 2/month

### ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³µ
- âœ… Real-time CTR insights
- âœ… Cost < $500/month (AWS)
- âœ… Scalable to 10x traffic
- âœ… Audit-ready compliance

---

## ë‹¤ìŒ ë‹¨ê³„

1. **ì´ ë¬¸ì„œë¥¼ íŒ€ê³¼ ê³µìœ **í•˜ê³  í”¼ë“œë°± ë°›ê¸°
2. **Phase 1ë¶€í„° ì‹œì‘** - Critical ë²„ê·¸ ìˆ˜ì •
3. **ì£¼ê°„ ì²´í¬í¬ì¸íŠ¸** ì„¤ì • - ì§„í–‰ ìƒí™© ì¶”ì 
4. **ë¸”ë¡œì»¤ ì‹ë³„** - ë§‰íˆëŠ” ë¶€ë¶„ ë¹ ë¥´ê²Œ í•´ê²°
5. **í•™ìŠµ ë¬¸ì„œí™”** - ë°°ìš´ ë‚´ìš© ADRë¡œ ê¸°ë¡

**ì§€ê¸ˆ ì‹œì‘í•˜ì„¸ìš”!** Phase 1ì˜ ì²« ë²ˆì§¸ task (CTRResultWindowProcessFunction ë²„ê·¸ ìˆ˜ì •)ë¶€í„° ì‹œì‘í•˜ë©´ ë©ë‹ˆë‹¤.

Good luck! ğŸš€
